//! Defensive Programming Integration Test
//!
//! Simple integration test for defensive programming patterns in storage operations.
//! Uses basic assertions to validate that defensive checks work correctly during
//! storage engine operations without relying on complex testing frameworks.
//!
//! This test validates:
//! - Basic input validation catches invalid parameters
//! - Memory safety patterns work correctly
//! - Performance remains acceptable with defensive checks
//! - Error handling provides useful information

const std = @import("std");

const types = @import("../../core/types.zig");
const storage = @import("../../storage/engine.zig");
const config = @import("../../storage/config.zig");
const test_harness = @import("../harness.zig");
const simulation_vfs = @import("../../sim/simulation_vfs.zig");
const defensive = @import("../../testing/defensive.zig");
const memory = @import("../../core/memory.zig");

const expect = std.testing.expect;
const expectEqual = std.testing.expectEqual;
const expectError = std.testing.expectError;
const ArrayList = std.ArrayList;

const ContextBlock = types.ContextBlock;
const BlockId = types.BlockId;
const StorageEngine = storage.StorageEngine;
const Config = config.Config;
const production_vfs = @import("../../core/production_vfs.zig");
const ProductionVFS = production_vfs.ProductionVFS;

/// Helper function to handle put_block with backpressure management
fn put_block_with_backpressure(engine: *StorageEngine, block: ContextBlock) !void {
    var retries: u8 = 0;
    const max_retries = 3;

    while (retries <= max_retries) {
        engine.put_block(block) catch |err| {
            // Handle write backpressure due to compaction throttling
            if (err == error.WriteStalled or err == error.WriteBlocked) {
                if (retries >= max_retries) {
                    return error.WriteStalled; // Give up after max retries
                }

                // Force flush memtable and allow time for compaction
                engine.flush_memtable_to_sstable() catch {};

                // Exponential backoff: 1ms, 2ms, 4ms
                const delay_ms = @as(u64, 1) << @intCast(retries);
                std.Thread.sleep(delay_ms * 1000000);

                retries += 1;
                continue;
            } else {
                return err;
            }
        };
        return; // Success
    }
}

test "storage engine basic defensive operations" {
    // Clean up any existing test directory to prevent file conflicts
    const test_dir = "/tmp/kausaldb-tests/defensive_basic";
    std.fs.cwd().deleteTree(test_dir) catch {};

    var prod_vfs = try std.testing.allocator.create(ProductionVFS);
    defer std.testing.allocator.destroy(prod_vfs);
    prod_vfs.* = ProductionVFS.init(std.testing.allocator);
    defer prod_vfs.deinit();

    var engine = try StorageEngine.init_default(std.testing.allocator, prod_vfs.vfs(), test_dir);
    defer engine.deinit();

    try engine.startup();
    defer engine.shutdown() catch {};

    // Test normal operation works
    const test_block = ContextBlock{
        .id = BlockId{ .bytes = [_]u8{ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16 } },
        .content = "test_content",
        .source_uri = "test://defensive/basic",
        .version = 1,
        .metadata_json = "{}",
    };

    try put_block_with_backpressure(&engine, test_block);

    // Verify block is retrievable
    const found = try engine.find_block(test_block.id, .temporary);
    try expect(found != null);
    if (found) |block| {
        // Content comparison not available with .temporary ownership
        _ = block; // Acknowledge the block exists
    }

    // Verify block count is correct
    const count = @as(u32, @intCast(engine.memtable_manager.block_index.blocks.count()));
    try expectEqual(@as(u32, 1), count);
}

test "storage engine input validation" {
    // Clean up any existing test directory to prevent file conflicts
    const test_dir = "/tmp/kausaldb-tests/defensive_validation";
    std.fs.cwd().deleteTree(test_dir) catch {};

    var prod_vfs = try std.testing.allocator.create(ProductionVFS);
    defer std.testing.allocator.destroy(prod_vfs);
    prod_vfs.* = ProductionVFS.init(std.testing.allocator);
    defer prod_vfs.deinit();

    var engine = try StorageEngine.init_default(std.testing.allocator, prod_vfs.vfs(), test_dir);
    defer engine.deinit();

    try engine.startup();
    defer engine.shutdown() catch {};

    // Test with valid block
    const valid_block = ContextBlock{
        .id = BlockId{ .bytes = [_]u8{42} ** 16 },
        .content = "valid_content",
        .source_uri = "test://valid",
        .version = 1,
        .metadata_json = "{}",
    };

    try put_block_with_backpressure(&engine, valid_block);

    // Verify it was stored
    const retrieved = try engine.find_block(valid_block.id, .temporary);
    try expect(retrieved != null);
}

test "storage engine memory behavior under load" {
    // Clean up any existing test directory to prevent file conflicts
    const test_dir = "/tmp/kausaldb-tests/defensive_memory";
    std.fs.cwd().deleteTree(test_dir) catch {};

    var prod_vfs = try std.testing.allocator.create(ProductionVFS);
    defer std.testing.allocator.destroy(prod_vfs);
    prod_vfs.* = ProductionVFS.init(std.testing.allocator);
    defer prod_vfs.deinit();

    var engine = try StorageEngine.init_default(std.testing.allocator, prod_vfs.vfs(), test_dir);
    defer engine.deinit();

    try engine.startup();
    defer engine.shutdown() catch {};

    // Simulate memory behavior under load
    const iterations: u32 = 100;
    var allocated_contents = std.array_list.Managed([]u8).init(std.testing.allocator);
    defer {
        for (allocated_contents.items) |content| {
            std.testing.allocator.free(content);
        }
        allocated_contents.deinit();
    }

    for (0..iterations) |i| {
        var id_bytes: [16]u8 = undefined;
        // Ensure non-zero ID by adding 1 to avoid potential validation issues
        std.mem.writeInt(u128, &id_bytes, @intCast(i + 1), .big);

        const content = try std.fmt.allocPrint(std.testing.allocator, "test_content_{}", .{i});
        try allocated_contents.append(content);

        const test_block = ContextBlock{
            .id = BlockId{ .bytes = id_bytes },
            .content = content,
            .source_uri = "test://memory/load",
            .version = 1,
            .metadata_json = "{}",
        };

        put_block_with_backpressure(&engine, test_block) catch |err| {
            // For load testing, allow some writes to fail due to backpressure
            if (err == error.WriteStalled or err == error.WriteBlocked) {
                // Skip this write - this is acceptable for load testing
                continue;
            }
            return err;
        };

        // Periodically flush to test flush behavior and prevent excessive backpressure
        if (i % 10 == 0) {
            try engine.flush_memtable_to_sstable();
        }
    }

    // Verify final state - expect some blocks were written despite backpressure
    const final_count = @as(u32, @intCast(engine.memtable_manager.block_index.blocks.count()));
    try expect(final_count > 0);
    try expect(final_count <= iterations); // Should not exceed what we attempted

    // In load testing, backpressure skipping is expected behavior
    if (final_count < iterations) {
        std.debug.print("Load test completed: {} of {} blocks written (backpressure caused {} skips)\n", .{ final_count, iterations, iterations - final_count });
    }
}

test "storage engine performance with defensive checks" {
    // Clean up any existing test directory to prevent file conflicts
    const test_dir = "/tmp/kausaldb-tests/defensive_performance";
    std.fs.cwd().deleteTree(test_dir) catch {};

    var prod_vfs = try std.testing.allocator.create(ProductionVFS);
    defer std.testing.allocator.destroy(prod_vfs);
    prod_vfs.* = ProductionVFS.init(std.testing.allocator);
    defer prod_vfs.deinit();

    var engine = try StorageEngine.init_default(std.testing.allocator, prod_vfs.vfs(), test_dir);
    defer engine.deinit();

    try engine.startup();
    defer engine.shutdown() catch {};

    const iterations = 50;
    const start_time = std.time.nanoTimestamp();

    // Perform operations and measure time
    var i: u32 = 0;
    while (i < iterations) : (i += 1) {
        // Ensure non-zero ID to avoid validation errors
        var id_bytes = [_]u8{@as(u8, @intCast((i % 255) + 1))} ** 16;
        id_bytes[15] = @as(u8, @intCast(i % 256)); // Make each ID unique
        const test_block = ContextBlock{
            .id = BlockId{ .bytes = id_bytes },
            .content = "performance_test_content",
            .source_uri = "test://performance",
            .version = 1,
            .metadata_json = "{}",
        };

        try put_block_with_backpressure(&engine, test_block);

        // Verify read performance
        const found = try engine.find_block(test_block.id, .temporary);
        try expect(found != null);
    }

    const end_time = std.time.nanoTimestamp();
    const total_time = @as(u64, @intCast(end_time - start_time));
    const time_per_operation = total_time / iterations;

    // Performance should be reasonable (under 1ms per operation in debug mode)
    const is_debug = std.debug.runtime_safety;
    if (is_debug) {
        try expect(time_per_operation < 5_000_000); // 5ms in nanoseconds (generous for CI)
    } else {
        try expect(time_per_operation < 100_000); // 100Âµs in nanoseconds for release
    }

    const mode_name = if (is_debug) "Debug" else "Release";
    std.debug.print("Performance: {} ns/operation in {s} mode\n", .{ time_per_operation, mode_name });
}

test "storage engine boundary conditions" {
    // Clean up any existing test directory to prevent file conflicts
    const test_dir = "/tmp/kausaldb-tests/defensive_boundary";
    std.fs.cwd().deleteTree(test_dir) catch {};

    var prod_vfs = try std.testing.allocator.create(ProductionVFS);
    defer std.testing.allocator.destroy(prod_vfs);
    prod_vfs.* = ProductionVFS.init(std.testing.allocator);
    defer prod_vfs.deinit();

    var engine = try StorageEngine.init_default(std.testing.allocator, prod_vfs.vfs(), test_dir);
    defer engine.deinit();

    try engine.startup();
    defer engine.shutdown() catch {};

    // Test with minimal content
    const minimal_block = ContextBlock{
        .id = BlockId{ .bytes = [_]u8{1} ** 16 },
        .content = "x", // Minimal content
        .source_uri = "test://minimal",
        .version = 1,
        .metadata_json = "{}",
    };

    try put_block_with_backpressure(&engine, minimal_block);

    // Test with larger content
    const large_content = try std.testing.allocator.alloc(u8, 32 * 1024); // 32KB
    defer std.testing.allocator.free(large_content);
    @memset(large_content, 'A');

    const large_block = ContextBlock{
        .id = BlockId{ .bytes = [_]u8{2} ** 16 },
        .content = large_content,
        .source_uri = "test://large",
        .version = 1,
        .metadata_json = "{}",
    };

    try engine.put_block(large_block);

    // Verify both blocks are accessible
    const found_minimal = try engine.find_block(minimal_block.id, .temporary);
    const found_large = try engine.find_block(large_block.id, .temporary);

    try expect(found_minimal != null);
    try expect(found_large != null);

    // Verify content integrity (limited with .temporary ownership)
    if (found_minimal) |block| {
        _ = block; // Content comparison not available with .temporary ownership
    }
    if (found_large) |block| {
        _ = block; // Content comparison not available with .temporary ownership
    }
}

test "storage engine error handling" {
    var prod_vfs = try std.testing.allocator.create(ProductionVFS);
    defer std.testing.allocator.destroy(prod_vfs);
    prod_vfs.* = ProductionVFS.init(std.testing.allocator);
    defer prod_vfs.deinit();

    // Test configuration validation
    const invalid_config = Config{
        .memtable_max_size = 512, // Too small, should fail validation
    };

    const init_result = StorageEngine.init(std.testing.allocator, prod_vfs.vfs(), "/tmp/kausaldb-tests/defensive_error", invalid_config);
    try expectError(config.ConfigError.MemtableMaxSizeTooSmall, init_result);
}

test "storage engine lifecycle management" {
    // Clean up any existing test directories to prevent file conflicts
    const base_dir = "/tmp/kausaldb-tests";
    std.fs.cwd().deleteTree(base_dir) catch {};

    var prod_vfs = try std.testing.allocator.create(ProductionVFS);
    defer std.testing.allocator.destroy(prod_vfs);
    prod_vfs.* = ProductionVFS.init(std.testing.allocator);
    defer prod_vfs.deinit();

    // Test multiple startup/shutdown cycles
    var cycle: u32 = 0;
    while (cycle < 3) : (cycle += 1) {
        const cycle_dir = try std.fmt.allocPrint(std.testing.allocator, "/tmp/kausaldb-tests/defensive_lifecycle_{}", .{cycle});
        defer std.testing.allocator.free(cycle_dir);

        var engine = try StorageEngine.init_default(std.testing.allocator, prod_vfs.vfs(), cycle_dir);
        defer engine.deinit();

        try engine.startup();

        // Ensure non-zero ID to avoid validation errors
        var id_bytes = [_]u8{@as(u8, @intCast(cycle + 1))} ** 16;
        id_bytes[15] = @as(u8, @intCast(cycle)); // Make each ID unique
        const test_block = ContextBlock{
            .id = BlockId{ .bytes = id_bytes },
            .content = try std.fmt.allocPrint(std.testing.allocator, "cycle_{}", .{cycle}),
            .source_uri = "test://lifecycle",
            .version = 1,
            .metadata_json = "{}",
        };
        defer std.testing.allocator.free(test_block.content);

        try put_block_with_backpressure(&engine, test_block);

        // Verify block is accessible
        const found = try engine.find_block(test_block.id, .temporary);
        try expect(found != null);

        engine.shutdown() catch {};
    }
}
